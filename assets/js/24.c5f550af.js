(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{280:function(e,n,t){},300:function(e,n,t){"use strict";t(280)},318:function(e,n,t){"use strict";t.r(n);t(300);var i=t(14),r=Object(i.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("ProfileSection",{attrs:{frontmatter:e.$page.frontmatter}}),e._v(" "),n("h2",{attrs:{id:"about-me"}},[e._v("About Me")]),e._v(" "),n("p",[e._v("Hi, I'm Zilyu, a second-year undergraduate student majoring in Artificial Intelligence at South China University of Technology. I'm currently working on my research project on AIGC supervised by "),n("a",{attrs:{href:"https://drliuqi.github.io/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof. Qi Liu"),n("OutboundLink")],1),e._v(" and "),n("a",{attrs:{href:"http://maple-lab.net/about.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof. Guo-Jun Qi"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("p",[n("span",{staticStyle:{color:"red"}},[e._v("Sincerely looking for Ph.D. positions for fall 2026 and internships opportunities in for Summer 2024.")])]),e._v(" "),n("h3",{attrs:{id:"research-interests"}},[e._v("Research Interests")]),e._v(" "),n("p",[e._v("Computer Vision, Multimodal Learning, AIGC, Diffusion Model etc.")]),e._v(" "),n("h2",{attrs:{id:"news"}},[e._v("News")]),e._v(" "),n("ul",[n("li",[e._v("[March 2024] Submit a paper focusing on the open-domain visual storytelling task to CVPR 2024 workshop.")])]),e._v(" "),n("h2",{attrs:{id:"education-experiences"}},[e._v("Education & Experiences")]),e._v(" "),n("ul",[n("li",[n("strong",[e._v("South China University of Technology (SCUT)")]),n("br"),e._v("\nUndergraduate major in Artificial Intelligence, 2022.9 - 2026.6 (expected)"),n("br")]),e._v(" "),n("li",[n("strong",[e._v("Multimodal Computing and Emotional Interaction Lab in SCUT")]),n("br"),e._v("\nResearch intern on AIGC supervised by "),n("a",{attrs:{href:"https://drliuqi.github.io/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof. Qi Liu"),n("OutboundLink")],1),e._v(", 2023.11 - present"),n("br")]),e._v(" "),n("li",[n("strong",[e._v("Machine Perception and Learning (MAPLE) Lab in Westlake University")]),n("br"),e._v("\nResearch intern on AIGC supervised by "),n("a",{attrs:{href:"http://maple-lab.net/about.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof. Guo-Jun Qi"),n("OutboundLink")],1),e._v(", 2024.3 - present"),n("br")]),e._v(" "),n("li",[n("strong",[e._v("Undergraduate Student Robotic Lab (ROBOTIC) in SCUT")]),n("br"),e._v("\nMember of machine vision team participating in ROBOCON competition, 2022.12 - 2023.6"),n("br")])]),e._v(" "),n("h2",{attrs:{id:"papers-and-projects"}},[e._v("Papers and Projects")]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/OpenStory.png",hideBorder:"true"}},[n("p",[n("span",{staticStyle:{"font-size":"1.1em"}},[n("strong",[e._v("OpenStory: A Large-Scale Open-Domain Dataset for Subject-Driven Visual Storytelling")])])]),e._v(" "),n("p",[e._v("Zilyu Ye*, Jinxiu Liu*, Jinjin Cao, Zhiyang Chen, Ziwei Xuan, Mingyuan Zhou, Qi Liu, Guo-Jun Qi")]),e._v(" "),n("p",[e._v("Recently, the advancement and evolution of genera-tive AI have been highly compelling. In this paper, wepresent OpenStory, a large-scale dataset tailored for train-ing subject-focused story visualization models to generatecoherent and contextually relevant visual narratives. Ad-dressing the challenges of maintaining subject continuityacross frames and capturing compelling narratives, We propose an innovative pipeline that automates the extraction ofkeyframes from open-domain videos. It ingeniously employsvision-language models to generate descriptive captions,which are then refined by a large language model to ensurenarrative flow and coherence. Furthermore, advanced sub-ject masking techniques are applied to isolate and segmentthe primary subjects. Derived from diverse video sources,including YouTube and existing datasets, OpenStory offersa comprehensive open-domain resource, surpassing priordatasets confined to specific scenarios. With automatedcaptioning instead of manual annotation, high-resolutionimagery optimized for subject count per frame, and exten-sive frame sequences ensuring consistent subjects for tem-poral modeling, OpenStory establishes itself as an invalu-able benchmark. It facilitates advancements in subject-focused story visualization, enabling the training of modelscapable of comprehending and generating intricate multi-modal narratives from extensive visual and textual inputs.")]),e._v(" "),n("p",[e._v("VDU@CVPR 2024 workshop, "),n("strong",[e._v("oral")])])]),e._v(" "),n("h2",{attrs:{id:"awards-honors"}},[e._v("Awards & Honors")]),e._v(" "),n("ul",[n("li",[e._v("SCUT undergraduate scholoarship, 2022 -- Third Prize")]),e._v(" "),n("li",[e._v("Excellent Group Enterprise Scholarship, SCUT, 2023 -- Third Prize")]),e._v(" "),n("li",[e._v("Asia and Pacific Mathematical Modeling Contest —Third Prize")]),e._v(" "),n("li",[e._v("National College Student Robot Contest (ROBOCON) —Third Prize")]),e._v(" "),n("li",[e._v("National Undergraduate Mathematical Modeling Contest In Guangdong Province — Second Prize")]),e._v(" "),n("li",[e._v('SCUT Future Technology Institute "Alibaba Cloud Cup" Programming Competition — Third Prize')])])],1)}),[],!1,null,null,null);n.default=r.exports}}]);